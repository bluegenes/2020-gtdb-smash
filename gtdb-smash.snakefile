"""
Author: N. Tessa Pierce, UC Davis Lab for Data Intensive Biology
Run: snakemake -s gtdb-smash.snakefile --use-conda
"""

import os
import glob
import pandas as pd
#lineage_csv=config["lineage_csv"]

output_extensions = ["sig"]
output_targets, nucleotide_targets, translate_targets, protein_targets = [],[],[],[]

out_dir = config.get("out_dir", "output-gtdb-smash")
data_dir= os.path.join(out_dir, "data")
logs_dir = os.path.join(out_dir, "logs")
envs_dir = "envs"
compute_dir = os.path.join(out_dir, "compute")
index_dir = os.path.join(out_dir, "sencha_index")
translate_dir = os.path.join(out_dir, "sencha_translate")

# databases
gtdb_datadir = config.get("gtdb_datadir", "/home/ctbrown/gtdbtk/release89/fastani/database")
gtdb_protein_datadir = config.get("gtdb_protein_datadir", "")
gtdb_rna_datadir = config.get("gtdb_rna_datadir", "")

translate = config.get("translate", False)
protein_input = config.get("protein_input", False)
encoding_info = config["sourmash_params"]["encoding"]

genome_lineage_list =config["genome_lineage_list"] # generated by extract-robust-lineages
genome_extension = "_genomic.fna.gz"
genome_lineages = [line.strip().split(genome_extension)[0] for line in open(genome_lineage_list)]#[:100]
g_acc =  set([g.rsplit(".",1)[0] for g in genome_lineages])

# 1. filenames are ACCESSION.1_genomic.fna.gz, so the sample name is ACCESSION.1, not ACCESSION
# are there ever multiple filenames per accession?

samples_csv = config["samples_csv"]
genome_info = pd.read_csv(samples_csv, sep="\t")
acc= set(genome_info["accession"].tolist())
filename_info = pd.read_csv("gtdb-lineages.csv")
import pdb;pdb.set_trace() # not exactly the same, sigh.
subDF = filename_info.accession.isin(acc)
filenames = filename_info.loc["filename"]


## build sencha targets
reference_info = config["sencha_params"].get("peptide_references")
refnames = []
default_tablesize = "1e8"

# include tablesize in reference name
for ref, info in reference_info.items():
    tablesize = info.get("tablesize", default_tablesize)
    refnames.append(f"{ref}_t{tablesize}")

sencha_extensions = ["codingpep.fa", "noncoding.fa", "lowcomplexnucl.fa", "csv", "json"]
alphabets_to_run = config["sencha_params"]["alphabet"] # dictionary of alphabets to run, corresponding ksizes
# build output files based on config
sencha_targets=[]
for alpha, alpha_info in alphabets_to_run.items():
    sencha_targets+=expand(os.path.join(translate_dir, "{sample}_{alphabet}_k{k}_ref{refname}_jacc{thresh}.{ext}"), sample=genome_lineages, alphabet=alpha, k=alpha_info["ksizes"], refname=refnames, ext=sencha_extensions, thresh=alpha_info["jaccard_threshold"])
    sencha_targets+=expand(os.path.join(index_dir, "ref{refname}_{alphabet}_k{k}.index"), refname=refnames, alphabet=alpha, k=alpha_info["ksizes"])
    if config.get("sencha_sigs", False):
        for encoding in ["protein", "dayhoff", "hp"]:
            sencha_targets+=expand(os.path.join(compute_dir, "sencha_translated", "{sample}_{alphabet}_k{ksize}_ref{refname}_jacc{thresh}.sourmash_{encoding}_scaled{scaled}_pk{sourmash_k}.{ext}"), sample=genome_lineages, alphabet=alpha, ksize=alpha_info["ksizes"], refname=refnames, thresh=alpha_info["jaccard_threshold"], encoding=encoding, scaled=encoding_info[encoding]["scaled"],sourmash_k=encoding_info[encoding]["ksize"], ext=output_extensions)
## build sourmash targets
nucleotide_encodings = [] 
if config.get("genome_sigs", True):
    nucleotide_encodings+=["dna"]
elif config.get("rna_sigs", False):
    nucleotide_encodings+=["rna"]

for enc in nucleotide_encodings:
    nucleotide_targets += expand(os.path.join(compute_dir, "{encoding}", "{sample}_{encoding}_scaled{scaled}_k{k}.{ext}"), sample = genome_lineages, encoding=enc, scaled=encoding_info[enc]["scaled"], k=encoding_info[enc]["ksize"], ext=output_extensions)
    if translate: # just for kicks, translating dna seqs too
        for encoding in ["protein", "dayhoff", "hp"]:
            # ksizes must be multiplied by 3 (give nucleotide ksizes to sourmash)
            translate_ksizes = [int(k)*3 for k in encoding_info[encoding]["ksize"]]
            translate_targets+=expand(os.path.join(compute_dir, f"{enc}","{sample}_{encoding}_scaled{scaled}_k{k}.{ext}"), sample = genome_lineages, encoding=encoding, scaled=encoding_info[encoding]["scaled"], k=translate_ksizes, ext=output_extensions)

if protein_input:
    for encoding in ["protein", "dayhoff", "hp"]:
        protein_targets+=expand(os.path.join(compute_dir,"protein","{sample}_{encoding}_scaled{scaled}_pk{k}.{ext}"), sample = genome_lineages, encoding=encoding, scaled=encoding_info[encoding]["scaled"],k=encoding_info[encoding]["ksize"], ext=output_extensions)

output_targets = nucleotide_targets + protein_targets + translate_targets #+ sencha_targets

rule all:
    input: output_targets

def find_genome_file(w):
    return glob.glob(os.path.join(gtdb_datadir, "*", f"*{w.sample}*"))

rule sourmash_compute_dna:
    input: find_genome_file
    output: os.path.join(compute_dir, "dna", "{sample}_{encoding}_scaled{scaled}_k{k}.sig")
    params:
        k= lambda w: w.k,
        scaled= lambda w: w.scaled,
        compute_moltypes= lambda w: w.encoding,
        input_is_protein=False,
        track_abundance=True,
    threads: 1
    resources:
        #mem_mb=3000,
        mem_mb=lambda wildcards, attempt: attempt *3000,
        runtime=600,
    log: os.path.join(logs_dir, "sourmash", "{sample}_{encoding}_scaled{scaled}_k{k}.dna.compute.log")
    benchmark: os.path.join(logs_dir, "sourmash", "{sample}_{encoding}_scaled{scaled}_k{k}.dna.compute.benchmark")
    conda: "envs/sourmash3.3.yml"
    script: "scripts/sourmash-compute.wrapper.py"

def find_corresponding_protein_file(w):
    return glob.glob(os.path.join(gtdb_protein_datadir, "*", f"*{w.sample}*"))

rule sourmash_compute_protein:
    input: find_corresponding_protein_file 
    output: os.path.join(compute_dir, "protein", "{sample}_{encoding}_scaled{scaled}_pk{k}.sig")
    params:
        k= lambda w: w.k,
        scaled= lambda w: w.scaled,
        compute_moltypes= lambda w: w.encoding,
        input_is_protein=True,
        track_abundance=True,
    threads: 1
    resources:
        #mem_mb=3000,
        mem_mb=lambda wildcards, attempt: attempt *3000,
        runtime=600,
    log: os.path.join(logs_dir, "sourmash", "{sample}_{encoding}_scaled{scaled}_k{k}.protein.compute.log")
    benchmark: os.path.join(logs_dir, "sourmash", "{sample}_{encoding}_scaled{scaled}_k{k}.protein.compute.benchmark")
    conda: "envs/sourmash3.3.yml"
    script: "scripts/sourmash-compute.wrapper.py"

def find_corresponding_rna_file(w):
    return glob.glob(os.path.join(gtdb_rna_datadir, "*", f"*{w.sample}*"))

# "rna" is not sourmash-friendly
moltype_map = {"rna": "dna", "dna": "dna", "protein":"protein", "dayhoff": "dayhoff", "hp":"hp"}

rule sourmash_compute_rna:
    input: find_corresponding_rna_file 
    output: os.path.join(compute_dir, "rna", "{sample}_{encoding}_scaled{scaled}_k{k}.sig")
    params:
        k= lambda w: w.k,
        scaled= lambda w: w.scaled,
        compute_moltypes= lambda w: moltype_map[w.encoding],
        input_is_protein=False,
        track_abundance=True,
    threads: 1
    resources:
        #mem_mb=3000,
        mem_mb=lambda wildcards, attempt: attempt *3000,
        runtime=600,
    log: os.path.join(logs_dir, "sourmash", "{sample}_{encoding}_scaled{scaled}_k{k}.rna.compute.log")
    benchmark: os.path.join(logs_dir, "sourmash", "{sample}_{encoding}_scaled{scaled}_k{k}.rna.compute.benchmark")
    conda: "envs/sourmash3.3.yml"
    script: "scripts/sourmash-compute.wrapper.py"

# abbreviate "hydrophobic-polar" as hp in filenames. Need full alpha name for sencha code
alpha_abbreviations = {"hp": "hydrophobic-polar", "protein": "protein", "dayhoff": "dayhoff"}

rule sencha_index_from_peptide_dir:
    output: os.path.join(index_dir, "ref{ref}_t{tablesize}_{alphabet}_k{ksize}.index"),
    log: os.path.join(logs_dir, "sencha_index", "ref{ref}_t{tablesize}_{alphabet}_k{ksize}.index.log")
    benchmark: os.path.join(logs_dir, "sencha_index", "ref{ref}_t{tablesize}_{alphabet}_k{ksize}.index.benchmark")
    params:
        alphabet=lambda w: alpha_abbreviations[w.alphabet],
        index_dir=gtdb_protein_datadir,
    threads: 1
    resources:
        mem_mb=lambda w:  reference_info[w.ref].get("index_memory", 20000),
        runtime=6000 # ~4 days
    wildcard_constraints:
        ref="\w+",
        alphabet="\w+",
        ksize="\d+",
    conda: os.path.join(envs_dir, "sencha-env.yml")
    shell:
        """
        sencha index --alphabet {params.alphabet} --peptide-ksize {wildcards.ksize} --tablesize {wildcards.tablesize} --save-as {output} --index-from-dir {params.index_dir} 2> {log}
        """

rule sencha_translate:
    input:
        fastq=find_corresponding_rna_file,
        index=rules.sencha_index_from_peptide_dir.output
    output:
        coding_prot=os.path.join(translate_dir, "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.codingpep.fa"),
        coding_nucl=os.path.join(translate_dir, "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.codingnucl.fa"),
        noncoding_nucl=os.path.join(translate_dir, "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.noncoding.fa"),
        low_complexity_prot=os.path.join(translate_dir, "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.lowcomplexprot.fa"),
        low_complexity_nucl=os.path.join(translate_dir, "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.lowcomplexnucl.fa"),
        csv=os.path.join(translate_dir, "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.csv"),
        json=os.path.join(translate_dir, "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.json"),
    log: os.path.join(logs_dir, "sencha_translate", "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.translate.log") #2>{log} err ("missing PEPTIDES file")
    benchmark: os.path.join(logs_dir, "sencha_translate", "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.translate.benchmark")
    params:
        alphabet=lambda w: alpha_abbreviations[w.alphabet],
    threads: 1
    resources:
        #mem_mb=lambda wildcards, attempt: attempt *10000, #10GB*attempt
        #mem_mb=50000,
        runtime=600,
        # subsampled files:
        #mem_mb=lambda wildcards, attempt: attempt *10000,
        mem_mb=lambda wildcards, attempt: attempt *6000,
        #runtime=60
    wildcard_constraints:
        ref="\w+",
        alphabet="\w+",
        ksize="\d+",
        jaccard_thresh="\d\.\d*",
    conda: os.path.join(envs_dir, "sencha-env.yml")
    shell:
        """
        sencha translate --verbose --peptides-are-bloom-filter --alphabet {params.alphabet} --peptide-ksize {wildcards.ksize} --jaccard-threshold {wildcards.jaccard_thresh} --noncoding-nucleotide-fasta {output.noncoding_nucl} --low-complexity-nucleotide-fasta {output.low_complexity_nucl} --coding-nucleotide-fasta {output.coding_nucl} --low-complexity-peptide-fasta {output.low_complexity_prot} --csv {output.csv} --json-summary {output.json} {input.index} {input.fastq} > {output.coding_prot} 2> {log}
        """
rule sourmash_compute_sencha_translated:
    input: rules.sencha_translate.output.coding_prot
    output: os.path.join(compute_dir, "sencha_translated", "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.sourmash_{encoding}_scaled{scaled}_pk{k}.sig")
    params:
        k= lambda w: w.k,
        scaled= lambda w: w.scaled,
        compute_moltypes= lambda w: w.encoding,
        input_is_protein=True,
        track_abundance=True,
    threads: 1
    resources:
        #mem_mb=3000,
        mem_mb=lambda wildcards, attempt: attempt *2000,
        runtime=600,
    wildcard_constraints:
        ref="\w+",
        alphabet="\w+",
        ksize="\d+",
        jaccard_thresh="\d\.\d*",
    log: os.path.join(logs_dir, "sourmash", "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.sourmash_{encoding}_scaled{scaled}_pk{k}.compute.log")
    benchmark: os.path.join(logs_dir, "sourmash", "{sample}_{alphabet}_k{ksize}_ref{ref}_t{tablesize}_jacc{jaccard_thresh}.sourmash_{encoding}_scaled{scaled}_pk{k}.compute.log")
    conda: "envs/sourmash3.3.yml"
    script: "scripts/sourmash-compute.wrapper.py"
